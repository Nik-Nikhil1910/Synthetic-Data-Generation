"""
SQL Exporter
============

Generates SQL DDL (CREATE TABLE) and INSERT statements.
Supports SQLite, PostgreSQL, and MySQL dialects.
"""

import os
from pathlib import Path
from typing import Any


# =============================================================================
# EXCEPTIONS
# =============================================================================

class SqlExportError(Exception):
    """Raised when SQL export fails."""
    pass


# =============================================================================
# TYPE MAPPINGS
# =============================================================================

# Stage 0 type -> SQL dialect type mapping
TYPE_MAPPING = {
    "sqlite": {
        "INTEGER": "INTEGER",
        "VARCHAR": "TEXT",
        "TEXT": "TEXT",
        "BOOLEAN": "INTEGER",
        "DATE": "TEXT",
        "DATETIME": "TEXT",
        "FLOAT": "REAL",
    },
    "postgresql": {
        "INTEGER": "INTEGER",
        "VARCHAR": "VARCHAR(255)",
        "TEXT": "TEXT",
        "BOOLEAN": "BOOLEAN",
        "DATE": "DATE",
        "DATETIME": "TIMESTAMP",
        "FLOAT": "DOUBLE PRECISION",
    },
    "mysql": {
        "INTEGER": "INT",
        "VARCHAR": "VARCHAR(255)",
        "TEXT": "TEXT",
        "BOOLEAN": "BOOLEAN",
        "DATE": "DATE",
        "DATETIME": "DATETIME",
        "FLOAT": "DOUBLE",
    },
}

# Valid dialects
VALID_DIALECTS = frozenset(TYPE_MAPPING.keys())


# =============================================================================
# PUBLIC INTERFACE
# =============================================================================

def export_to_sql(
    data_store: dict[str, list[dict]],
    schema: dict,
    output_dir: str,
    dialect: str = "sqlite"
) -> str:
    """
    Export data and schema to SQL script.
    
    Args:
        data_store: dict of table_name -> list of records.
        schema: Stage 0 schema (required for types/FKs).
        output_dir: Directory to write file to.
        dialect: 'sqlite', 'postgresql', or 'mysql'.
    
    Returns:
        Path to generated SQL file.
    
    Raises:
        SqlExportError: If export fails.
    """
    if dialect not in VALID_DIALECTS:
        raise SqlExportError(f"Invalid dialect '{dialect}'. Valid: {sorted(VALID_DIALECTS)}")
    
    if not data_store:
        raise SqlExportError("data_store is empty.")
    
    if not schema or "tables" not in schema:
        raise SqlExportError("Schema is required for SQL export.")
    
    # Ensure output directory exists
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Build schema lookup
    table_schemas = {t["name"]: t for t in schema["tables"]}
    
    # Determine execution order from schema dependencies
    execution_order = _get_execution_order(schema)
    
    # Generate SQL parts
    sql_parts = []
    
    # Header comment
    sql_parts.append(f"-- Auto-generated SQL script ({dialect})")
    sql_parts.append(f"-- Generated by: AI-Driven Synthetic Data Generator (Stage C)")
    sql_parts.append("")
    
    # DDL: CREATE TABLE statements
    sql_parts.append("-- ============================================")
    sql_parts.append("-- DDL: CREATE TABLE STATEMENTS")
    sql_parts.append("-- ============================================")
    sql_parts.append("")
    
    for table_name in execution_order:
        table_schema = table_schemas.get(table_name)
        if table_schema:
            ddl = _generate_create_table(table_schema, dialect)
            sql_parts.append(ddl)
            sql_parts.append("")
    
    # DML: INSERT statements
    sql_parts.append("-- ============================================")
    sql_parts.append("-- DML: INSERT STATEMENTS")
    sql_parts.append("-- ============================================")
    sql_parts.append("")
    
    for table_name in execution_order:
        records = data_store.get(table_name, [])
        table_schema = table_schemas.get(table_name)
        if records and table_schema:
            inserts = _generate_inserts(table_name, records, table_schema, dialect)
            sql_parts.append(inserts)
            sql_parts.append("")
    
    # Write to file
    file_path = output_path / f"export_{dialect}.sql"
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(sql_parts))
    except Exception as e:
        raise SqlExportError(f"Failed to write {file_path}: {e}") from e
    
    return str(file_path)


def export_all_dialects(
    data_store: dict[str, list[dict]],
    schema: dict,
    output_dir: str
) -> list[str]:
    """
    Export to all supported SQL dialects.
    
    Returns:
        List of generated file paths.
    """
    files = []
    for dialect in VALID_DIALECTS:
        file_path = export_to_sql(data_store, schema, output_dir, dialect)
        files.append(file_path)
    return files


# =============================================================================
# DDL GENERATION
# =============================================================================

def _generate_create_table(table_schema: dict, dialect: str) -> str:
    """Generate CREATE TABLE statement for a table."""
    table_name = table_schema["name"]
    columns = table_schema.get("columns", [])
    foreign_keys = table_schema.get("foreign_keys", [])
    
    type_map = TYPE_MAPPING[dialect]
    
    lines = [f"CREATE TABLE {_quote_identifier(table_name, dialect)} ("]
    
    column_defs = []
    pk_columns = []
    
    for col in columns:
        col_name = col["name"]
        col_type = col["type"]
        is_pk = col.get("is_pk", False)
        is_nullable = col.get("is_nullable", True)
        
        # Map type
        sql_type = type_map.get(col_type, "TEXT")
        
        # Build column definition
        col_def = f"    {_quote_identifier(col_name, dialect)} {sql_type}"
        
        if not is_nullable:
            col_def += " NOT NULL"
        
        column_defs.append(col_def)
        
        if is_pk:
            pk_columns.append(col_name)
    
    # Add PRIMARY KEY constraint
    if pk_columns:
        pk_cols = ", ".join(_quote_identifier(c, dialect) for c in pk_columns)
        column_defs.append(f"    PRIMARY KEY ({pk_cols})")
    
    # Add FOREIGN KEY constraints
    for fk in foreign_keys:
        fk_col = fk["column"]
        ref_table = fk["references_table"]
        ref_col = fk["references_column"]
        
        fk_def = (
            f"    FOREIGN KEY ({_quote_identifier(fk_col, dialect)}) "
            f"REFERENCES {_quote_identifier(ref_table, dialect)}"
            f"({_quote_identifier(ref_col, dialect)})"
        )
        column_defs.append(fk_def)
    
    lines.append(",\n".join(column_defs))
    lines.append(");")
    
    return "\n".join(lines)


# =============================================================================
# DML GENERATION
# =============================================================================

def _generate_inserts(
    table_name: str,
    records: list[dict],
    table_schema: dict,
    dialect: str
) -> str:
    """Generate INSERT statements for records."""
    if not records:
        return ""
    
    # Get column order from schema
    columns = [col["name"] for col in table_schema.get("columns", [])]
    
    lines = [f"-- {table_name}: {len(records)} rows"]
    
    for record in records:
        values = []
        for col_name in columns:
            value = record.get(col_name)
            values.append(_format_value(value, dialect))
        
        col_list = ", ".join(_quote_identifier(c, dialect) for c in columns)
        val_list = ", ".join(values)
        
        lines.append(
            f"INSERT INTO {_quote_identifier(table_name, dialect)} "
            f"({col_list}) VALUES ({val_list});"
        )
    
    return "\n".join(lines)


# =============================================================================
# HELPERS
# =============================================================================

def _get_execution_order(schema: dict) -> list[str]:
    """Determine table order based on FK dependencies (parents first)."""
    tables = schema.get("tables", [])
    table_names = {t["name"] for t in tables}
    
    # Build dependency graph
    dependencies: dict[str, set[str]] = {t["name"]: set() for t in tables}
    
    for table in tables:
        for fk in table.get("foreign_keys", []):
            ref_table = fk.get("references_table", "")
            if ref_table in table_names:
                dependencies[table["name"]].add(ref_table)
    
    # Topological sort (Kahn's algorithm)
    in_degree = {name: len(deps) for name, deps in dependencies.items()}
    queue = [name for name, degree in in_degree.items() if degree == 0]
    result = []
    
    while queue:
        # Sort for determinism
        queue.sort()
        current = queue.pop(0)
        result.append(current)
        
        for name, deps in dependencies.items():
            if current in deps:
                in_degree[name] -= 1
                if in_degree[name] == 0:
                    queue.append(name)
    
    return result


def _quote_identifier(name: str, dialect: str) -> str:
    """Quote identifier based on dialect."""
    if dialect == "mysql":
        return f"`{name}`"
    elif dialect == "postgresql":
        return f'"{name}"'
    else:  # sqlite
        return f'"{name}"'


def _format_value(value: Any, dialect: str) -> str:
    """Format a value for SQL INSERT."""
    if value is None:
        return "NULL"
    elif isinstance(value, bool):
        if dialect == "sqlite":
            return "1" if value else "0"
        else:
            return "TRUE" if value else "FALSE"
    elif isinstance(value, (int, float)):
        return str(value)
    elif isinstance(value, str):
        return _escape_string(value, dialect)
    elif hasattr(value, "isoformat"):
        # datetime/date objects
        return _escape_string(value.isoformat(), dialect)
    else:
        return _escape_string(str(value), dialect)


def _escape_string(s: str, dialect: str) -> str:
    """Escape string for SQL."""
    # Escape single quotes by doubling them
    escaped = s.replace("'", "''")
    return f"'{escaped}'"
